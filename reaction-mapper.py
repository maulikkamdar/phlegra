import os, json, re, argparse
from copy import deepcopy

# This script uses three terminologies in RDF triples -- MESH, ICD10CM and OMIM to map the cuis to the diseases generated by the graph 
# It uses the input terminologies and generates three dictionaries CUI -> MESH_IDs, ICD10_IDs -> CUI, OMIM_IDs -> CUI
# and then translates all diseases from the graph (that may be referenced using ICD10 or OMIM Ids) to its MESH IDs and CUIs
# It then reads the mapped reactions (and indications) as well as unmapped reactions generated from reaction-term.py files
# @TODO Now that I can translate RDF triples to HDT files, it might make sense to combine this with the query federation
# First run reaction-term.py to map FAERS reactions encoded in MEDDRA to MESH terms -- this will generate three files - reaction_meddra.tsv, unmatched_reactions.tsv, and unmatched_mesh.tsv
# @TODO It might make sense to merge reaction-term.py and reaction-mapper.py
# python reaction-mapper.py -i all_dises2.tsv -mesh ../stage_rdf_dump/ontologies/MESH--8 -icd10cm ../stage_rdf_dump/ontologies/ICD10CM--5 -omim ../stage_rdf_dump/ontologies/OMIM--6 -fmap to_use_data_files/map -o to_use_data_files/nmap -type reaction
# python to_use_scripts/reaction-mapper.py -i all_dises2.tsv -mesh ../stage_rdf_dump/ontologies/MESH--8 -icd10cm ../stage_rdf_dump/ontologies/ICD10CM--5 -omim ../stage_rdf_dump/ontologies/OMIM--6 -fmap to_use_data_files/map -o to_use_data_files/nmap -type indication
# These mappings generation procedures are a mess ... 

all_dises = {}
mesh_cuis = {}
unfound_terms = []
mesh_ids = {}

def parse_args():
	p = argparse.ArgumentParser(description='Map the Phenotypes in the graph, generated from drug-network.py')
	p.add_argument('-i', '--input', help='Name of the file containing the reactions/indications', default="reaction-faers.tsv")
	p.add_argument('-mesh', '--mesh', help='Location of the N-triple MESH terminology', default="../data/MESH--8")
	p.add_argument('-icd10cm', '--icd10cm', help='Location of the N-triple ICD10CM terminology', default="../data/ICD10CM--5")
	p.add_argument('-omim', '--omim', help='Location of the N-triple OMIM terminology', default="../data/OMIM--6")
	p.add_argument('-fmap', '--fmap', help='prefix of the FAERS mapping files that you generated from reaction-term.py', default="results")
	p.add_argument('-o', '--prefix', help='prefix of the output files (with directory)', default="results")
	p.add_argument('-type', '--input_type', help='specify if reaction or indication', default="reaction")
	return p.parse_args()

def populate_dict(terminology):
	'''this function populates a dictionary mapping ids to cui'''
	dict_mapping = {}
	termfile = open(terminology)
	termlines = termfile.readlines()
	termfile.close()
	for k in range(len(termlines)):
		term_parts = termlines[k].strip().split()
		if "cui" in term_parts[1]:
			term_id_parts = term_parts[0][1:-1].split("/")
			dict_mapping[term_id_parts[len(term_id_parts)-1]] = term_parts[2].split("^^")[0][1:-1]
	return dict_mapping

def capture_name(given_id, title_str):
	title_parts = title_str.split(":-:")
	for title in title_parts:
		refined_title = title.split("@")[0][1:-1]
		refined_title_parts = refined_title.split(";")
		for updated_title in refined_title_parts:
			all_dises[updated_title.strip()] = given_id

def unfound_term_collector(filename):
	reactionfile = open(filename)
	reactionlines = reactionfile.readlines()
	reactionfile.close()
	for k in range(len(reactionlines)):
		reaction_parts = reactionlines[k].strip().split("\t")
		unfound_terms.append(reaction_parts[0].strip())

def get_faers_mapper(reaction):
	reaction_name = re.sub('[^0-9a-zA-Z]+', " ", reaction)
	reaction_name = ' '.join(reaction_name.split())
	ldfaers = None
	ess_chars = []
	if reaction_name.lower() in all_dises:
		ldfaers = all_dises[reaction_name.lower()]
		ess_chars.append(reaction_name.lower())
		print "wohoo " + reaction_name.lower()
	else: # do longest match
		reaction_parts = reaction_name.split()
		for m in range(len(reaction_parts)):
			for k in reversed(range(1, len(reaction_parts))):		
				try_name = " ".join(reaction_parts[m:m+k])
				#print try_name
				if try_name.lower() in all_dises:
					ldfaers = all_dises[try_name.lower()]
					ess_chars = [try_name.lower()]
					break
			if ldfaers:
				break
	if not ldfaers:
		ess_chars.extend([name.lower() for name in reaction_name.split()])
	return (ldfaers, ess_chars)

def query_dict(reaction_parts, dict_mapping, index):
	dem = reaction_parts[index].split(";")
	for term_id in dem:
		if term_id in dict_mapping:
			cui = dict_mapping[term_id]
			if cui in mesh_cuis:
				mesh_id = mesh_cuis[cui]
				mesh_ids[mesh_id] = reaction_parts[0]
				return mesh_id
	return None

def main():
	args = parse_args()
	
	mesh_file = open(args.mesh)
	meshlines = mesh_file.readlines()
	mesh_file.close()

	for k in range(len(meshlines)):
		mesh_parts = meshlines[k].strip().split()
		if "cui" in mesh_parts[1]:
			cui = mesh_parts[2].split("^^")[0][1:-1]
			id_parts = mesh_parts[0][1:-1].split("/")
			mesh_cuis[cui] = id_parts[len(id_parts)-1]

	icd10_ids = populate_dict(args.icd10cm)
	omim_ids = populate_dict(args.omim)
	dise_ids = {}
	#all_dises = {}
	#mesh_ids = {}
	updated_file = open(args.prefix + "_" + args.input_type + "_updated_file.tsv", "w+")

	reactionfile = open(args.input)
	reactionlines = reactionfile.readlines()
	reactionfile.close()

	for k in range(len(reactionlines)):
		reaction_parts = reactionlines[k].strip().split("\t")
		capture_name(reaction_parts[0], reaction_parts[1])
		mesh_id = None
		if len(reaction_parts) > 4:
			updated_file.write(reaction_parts[0] + "\t" + reaction_parts[1] + "\t" + reaction_parts[4] + "\n")
			mesh_ids[reaction_parts[4]] = reaction_parts[0]
			continue
		elif len(reaction_parts) > 2:
			print "No mesh here" + reactionlines[k]
			if reaction_parts[2] != "":
				mesh_id = query_dict(reaction_parts, icd10_ids, 2)
				if mesh_id:
					updated_file.write(reaction_parts[0] + "\t" + reaction_parts[1] + "\t" + mesh_id + "\n")
					print "Mesh found using ICD " + mesh_id
					continue
			if len(reaction_parts) > 3 and reaction_parts[3] != "":
				mesh_id = query_dict(reaction_parts, omim_ids, 3)
				if mesh_id:
					updated_file.write(reaction_parts[0] + "\t" + reaction_parts[1] + "\t" + mesh_id + "\n")
					print "Mesh found using OMIM " + mesh_id

	print len(mesh_ids)
	updated_file.close()

	# first Locator file for the exact MEDDRA - MESH Mappings
	reactionfile = open(args.fmap + "_" + args.input_type + "-meddra.tsv")
	reactionlines = reactionfile.readlines()
	reactionfile.close()

	locatorfile = open(args.prefix + "_" + args.input_type + "_locatorfile0.tsv", "w+")

	for k in range(len(reactionlines)):
		reaction_parts = reactionlines[k].strip().split("\t")
		mesh_id_parts = reaction_parts[2].split("/")
		mesh_id = mesh_id_parts[len(mesh_id_parts)-1]
		if mesh_id in mesh_ids:
			locatorfile.write(reaction_parts[0] + "\t" + mesh_id + "\t" + mesh_ids[mesh_id] + "\n")
		else:
			unfound_terms.append(reaction_parts[0].strip())

	locatorfile.close()

	locatorfile = open(args.prefix + "_" + args.input_type + "_locatorfile1.tsv", "w+")
	unfound_term_collector(args.fmap + "_" + "unmatched-mesh-" + args.input_type + ".tsv")
	unfound_term_collector(args.fmap + "_" + "unmatched-" + args.input_type + ".tsv")

	dises = deepcopy(all_dises)
	for dise in dises:
		reaction_name = re.sub('[^0-9a-zA-Z]+', " ", dise)
		reaction_name = ' '.join(reaction_name.split())
		all_dises[reaction_name.lower()] = all_dises.pop(dise) 

	unknown_terms = {}
	for dise in unfound_terms:
		(ldfaers, ess_chars) = get_faers_mapper(dise)
		if ldfaers:
			locatorfile.write(dise + "\t" + ldfaers + "\t" + ";".join(ess_chars) + "\n")
		else:
			unknown_terms[dise] = ess_chars

	locatorfile.close()

	# move this to Utils somewhere ... 
	stopWords = set([
	    "a", "also", "although", "am", "an", "and", "are",
	    "as", "at", "back", "be", "became", "because", "become",
	    "becomes", "becoming", "been", "being", "bill", "both",
	    "bottom", "but", "by", "call", "can", "con",
	    "could", "de", "do", "done", "eg", "etc", "even", "ever", 
	    "find", "for", "found", "from", "get", "give", "go",
	    "had", "has", "have", "he", "her", "here", "hers", "herself", "him", "himself", "his",
	    "how", "however", "if", "in", "inc", 
	    "into", "is", "it", "its", "itself", "keep", "may", "me", "mine", "my", "myself", "name", "namely", "of", "onto", "our",
	    "ours", "ourselves", "please", "put", "should", "show", "such", "take", "that", "the", "their", "them",
	    "themselves", "these", "they", "this", "those", "though",
	    "thru", "to", "us", "via", "was", "we", "were", "what", "whatever", "when",
	    "whence", "whenever", "where", "whereafter", "whereas", "whereby",
	    "wherein", "whereupon", "wherever", "whether", "which", "whither",
	    "who", "whoever", "whom", "whose", "why", "will", "would", "yet", "you", "your", "yours", "yourself", "yourselves"])

	locatorfile = open(args.prefix + "_" + args.input_type + "_locatorfile2.tsv", "w+")
	for dise in unknown_terms:
		print unknown_terms[dise]
		matched_terms = {}
		for term in unknown_terms[dise]:
			if term in stopWords:
				continue
			matched_terms[term] = []
			for name in all_dises:
				if term in name:
					matched_terms[term].append(name)
		keys = deepcopy(matched_terms.keys())
		keys.sort(key=len, reverse=True)
		print keys[0]
		for name in matched_terms[keys[0]]:
			locatorfile.write(dise + "\t" + all_dises[name] + "\t" + name + "\t" + term + "\t" + ";".join(unknown_terms[dise]) + "\n")

	locatorfile.close()

if __name__ == "__main__":
	main()
